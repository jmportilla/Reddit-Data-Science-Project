{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!sudo pip install Pyro4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking gensim\n",
      "  Downloading gensim-0.12.1.tar.gz (2.3Mb): 2.3Mb downloaded\n",
      "  Running setup.py egg_info for package gensim\n",
      "    \n",
      "    warning: no files found matching '*.sh' under directory '.'\n",
      "    no previously-included directories found matching 'docs/src*'\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.3 in /usr/local/lib/python2.7/dist-packages (from gensim)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy>=0.7.0 in /usr/lib/python2.7/dist-packages (from gensim)\n",
      "Downloading/unpacking six>=1.2.0 (from gensim)\n",
      "  Downloading six-1.9.0.tar.gz\n",
      "  Running setup.py egg_info for package six\n",
      "    \n",
      "    no previously-included directories found matching 'documentation/_build'\n",
      "Downloading/unpacking smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.2.1.tar.gz\n",
      "  Running setup.py egg_info for package smart-open\n",
      "    \n",
      "Downloading/unpacking boto>=2.32 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto-2.38.0.tar.gz (1.4Mb): 1.4Mb downloaded\n",
      "  Running setup.py egg_info for package boto\n",
      "    \n",
      "    warning: no files found matching 'boto/mturk/test/*.doctest'\n",
      "    warning: no files found matching 'boto/mturk/test/.gitignore'\n",
      "Downloading/unpacking bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "  Running setup.py egg_info for package bz2file\n",
      "    \n",
      "Installing collected packages: gensim, six, smart-open, boto, bz2file\n",
      "  Running setup.py install for gensim\n",
      "    \n",
      "    warning: no files found matching '*.sh' under directory '.'\n",
      "    no previously-included directories found matching 'docs/src*'\n",
      "    building 'gensim.models.word2vec_inner' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/ubuntu/notebooks/build/gensim/gensim/models -I/usr/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -c ./gensim/models/word2vec_inner.c -o build/temp.linux-x86_64-2.7/./gensim/models/word2vec_inner.o\n",
      "    /usr/include/python2.7/numpy/__multiarray_api.h:1532:1: warning: ���_import_array��� defined but not used [-Wunused-function]\n",
      "    /usr/include/python2.7/numpy/__ufunc_api.h:226:1: warning: ���_import_umath��� defined but not used [-Wunused-function]\n",
      "    ./gensim/models/word2vec_inner.c: In function ���__pyx_pf_6gensim_6models_14word2vec_inner_train_sentence_sg.isra.14���:\n",
      "    ./gensim/models/word2vec_inner.c:2052:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3030:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1994:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3032:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1994:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3031:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1870:58: warning: ���__pyx_v_next_random��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3033:25: note: ���__pyx_v_next_random��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1664:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:3027:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c: In function ���__pyx_pf_6gensim_6models_14word2vec_inner_2train_sentence_cbow.isra.13���:\n",
      "    ./gensim/models/word2vec_inner.c:2764:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4043:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2706:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4045:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2706:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4044:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:1870:58: warning: ���__pyx_v_next_random��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4046:25: note: ���__pyx_v_next_random��� was declared here\n",
      "    ./gensim/models/word2vec_inner.c:2334:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/word2vec_inner.c:4040:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/./gensim/models/word2vec_inner.o -o build/lib.linux-x86_64-2.7/gensim/models/word2vec_inner.so\n",
      "    building 'gensim.models.doc2vec_inner' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/home/ubuntu/notebooks/build/gensim/gensim/models -I/usr/include/python2.7 -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -c ./gensim/models/doc2vec_inner.c -o build/temp.linux-x86_64-2.7/./gensim/models/doc2vec_inner.o\n",
      "    /usr/include/python2.7/numpy/__multiarray_api.h:1532:1: warning: ���_import_array��� defined but not used [-Wunused-function]\n",
      "    /usr/include/python2.7/numpy/__ufunc_api.h:226:1: warning: ���_import_umath��� defined but not used [-Wunused-function]\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_2train_document_dm.isra.12���:\n",
      "    ./gensim/models/doc2vec_inner.c:2166:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4363:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2108:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4365:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2108:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4364:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:1936:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4360:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_train_document_dbow.isra.13���:\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4053:99: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:4031:76: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c: In function ���__pyx_pf_6gensim_6models_13doc2vec_inner_4train_document_dm_concat.isra.11���:\n",
      "    ./gensim/models/doc2vec_inner.c:2545:65: warning: ���__pyx_v_syn1neg��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6065:52: note: ���__pyx_v_syn1neg��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2487:151: warning: ���__pyx_v_cum_table_len��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6067:25: note: ���__pyx_v_cum_table_len��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2487:151: warning: ���__pyx_v_cum_table��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6066:28: note: ���__pyx_v_cum_table��� was declared here\n",
      "    ./gensim/models/doc2vec_inner.c:2315:65: warning: ���__pyx_v_syn1��� may be used uninitialized in this function [-Wuninitialized]\n",
      "    ./gensim/models/doc2vec_inner.c:6062:52: note: ���__pyx_v_syn1��� was declared here\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/./gensim/models/doc2vec_inner.o -o build/lib.linux-x86_64-2.7/gensim/models/doc2vec_inner.so\n",
      "  Running setup.py install for six\n",
      "    \n",
      "    no previously-included directories found matching 'documentation/_build'\n",
      "  Running setup.py install for smart-open\n",
      "    \n",
      "  Found existing installation: boto 2.2.2\n",
      "    Uninstalling boto:\n",
      "      Successfully uninstalled boto\n",
      "  Running setup.py install for boto\n",
      "    \n",
      "    warning: no files found matching 'boto/mturk/test/*.doctest'\n",
      "    warning: no files found matching 'boto/mturk/test/.gitignore'\n",
      "    changing mode of build/scripts-2.7/sdbadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/elbadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cfadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/s3put from 644 to 755\n",
      "    changing mode of build/scripts-2.7/fetch_file from 644 to 755\n",
      "    changing mode of build/scripts-2.7/launch_instance from 644 to 755\n",
      "    changing mode of build/scripts-2.7/list_instances from 644 to 755\n",
      "    changing mode of build/scripts-2.7/taskadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/kill_instance from 644 to 755\n",
      "    changing mode of build/scripts-2.7/bundle_image from 644 to 755\n",
      "    changing mode of build/scripts-2.7/pyami_sendmail from 644 to 755\n",
      "    changing mode of build/scripts-2.7/lss3 from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cq from 644 to 755\n",
      "    changing mode of build/scripts-2.7/route53 from 644 to 755\n",
      "    changing mode of build/scripts-2.7/cwutil from 644 to 755\n",
      "    changing mode of build/scripts-2.7/instance_events from 644 to 755\n",
      "    changing mode of build/scripts-2.7/asadmin from 644 to 755\n",
      "    changing mode of build/scripts-2.7/glacier from 644 to 755\n",
      "    changing mode of build/scripts-2.7/mturk from 644 to 755\n",
      "    changing mode of build/scripts-2.7/dynamodb_dump from 644 to 755\n",
      "    changing mode of build/scripts-2.7/dynamodb_load from 644 to 755\n",
      "    changing mode of /usr/local/bin/instance_events to 755\n",
      "    changing mode of /usr/local/bin/s3put to 755\n",
      "    changing mode of /usr/local/bin/sdbadmin to 755\n",
      "    changing mode of /usr/local/bin/dynamodb_load to 755\n",
      "    changing mode of /usr/local/bin/fetch_file to 755\n",
      "    changing mode of /usr/local/bin/asadmin to 755\n",
      "    changing mode of /usr/local/bin/pyami_sendmail to 755\n",
      "    changing mode of /usr/local/bin/list_instances to 755\n",
      "    changing mode of /usr/local/bin/cfadmin to 755\n",
      "    changing mode of /usr/local/bin/lss3 to 755\n",
      "    changing mode of /usr/local/bin/launch_instance to 755\n",
      "    changing mode of /usr/local/bin/glacier to 755\n",
      "    changing mode of /usr/local/bin/bundle_image to 755\n",
      "    changing mode of /usr/local/bin/route53 to 755\n",
      "    changing mode of /usr/local/bin/elbadmin to 755\n",
      "    changing mode of /usr/local/bin/cwutil to 755\n",
      "    changing mode of /usr/local/bin/cq to 755\n",
      "    changing mode of /usr/local/bin/mturk to 755\n",
      "    changing mode of /usr/local/bin/dynamodb_dump to 755\n",
      "    changing mode of /usr/local/bin/kill_instance to 755\n",
      "    changing mode of /usr/local/bin/taskadmin to 755\n",
      "  Running setup.py install for bz2file\n",
      "    \n",
      "Successfully installed gensim six smart-open boto bz2file\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking nltk\n",
      "  Downloading nltk-3.0.4.tar.gz (1.0Mb): 1.0Mb downloaded\n",
      "  Running setup.py egg_info for package nltk\n",
      "    \n",
      "    warning: no files found matching 'Makefile' under directory '*.txt'\n",
      "    warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "Installing collected packages: nltk\n",
      "  Running setup.py install for nltk\n",
      "    \n",
      "    warning: no files found matching 'Makefile' under directory '*.txt'\n",
      "    warning: no previously-included files matching '*~' found anywhere in distribution\n",
      "Successfully installed nltk\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading/unpacking python-cjson\n",
      "  Downloading python-cjson-1.1.0.tar.gz\n",
      "  Running setup.py egg_info for package python-cjson\n",
      "    \n",
      "Installing collected packages: python-cjson\n",
      "  Running setup.py install for python-cjson\n",
      "    building 'cjson' extension\n",
      "    gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DMODULE_VERSION=1.1.0 -I/usr/include/python2.7 -c cjson.c -o build/temp.linux-x86_64-2.7/cjson.o\n",
      "    gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro build/temp.linux-x86_64-2.7/cjson.o -o build/lib.linux-x86_64-2.7/cjson.so\n",
      "    \n",
      "Successfully installed python-cjson\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install python-cjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'all-corpora'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package oanc_masc to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all-corpora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all-corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('[deleted]')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    INPUT: string of body text\n",
    "    OUTPUT: List of tokenized lower case words with stopwords removed\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Output tokenizes text and removes any stopwords and then outptus lowercased words\n",
    "    return [word.lower() for word in tokenizer.tokenize(text) if not word.lower() in stopwords]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cjson\n",
    "#from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence #,TaggedDocument\n",
    "#import gensim.models.doc2vec\n",
    "\n",
    "\n",
    "def reddit_comment_gen(pathway):\n",
    "    '''\n",
    "    INPUT: Pathway to database and num of comments to be generated. If everything is True, all comments returned.\n",
    "    OUTPUT: Generator label and tokenized comment\n",
    "\n",
    "    '''\n",
    "\n",
    "    ## Generate all labeled sentences from file\n",
    "\n",
    "    # \n",
    "    \n",
    "   \n",
    "        # Iterate through N JSON objects in file\n",
    "    with open(pathway) as myfile:\n",
    "        for item in myfile:\n",
    "            \n",
    "            \n",
    "            # put in try statement here\n",
    "\n",
    "            # Load the JSON object\n",
    "            json_object = cjson.decode(item)\n",
    "\n",
    "            # Clean and tokenize text\n",
    "            body = text_cleaner(json_object['body'])\n",
    "\n",
    "            # generate\n",
    "            yield LabeledSentence(body,[str(json_object['subreddit'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_model(pathway):\n",
    "\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    assert gensim.models.doc2vec.FAST_VERSION > -1\n",
    "\n",
    "    \n",
    "    d2v_reddit_model = Doc2Vec( dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "    d2v_reddit_model.train_words = False\n",
    "    \n",
    "    d2v_reddit_model.build_vocab(reddit_comment_gen(pathway)) \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    for epoch in range(10):\n",
    "\n",
    "        d2v_reddit_model.train(reddit_comment_gen(pathway))\n",
    "        d2v_reddit_model.alpha -= 0.002  # decrease the learning rate\n",
    "        d2v_reddit_model.min_alpha = d2v_reddit_model.alpha  # fix the learning rate, no decay\n",
    "\n",
    "    return d2v_reddit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import boto\n",
    "from boto.s3.connection import S3Connection\n",
    "\n",
    "# Get connection with keys (may need to input these keys)\n",
    "conn = S3Connection()\n",
    "# Connect to exsisting bucket\n",
    "mybucket = conn.get_bucket('redditdatascienceproject')\n",
    "\n",
    "# List keys in the bucket\n",
    "mybucket.list()\n",
    "\n",
    "# Use key to grab specific file from bucket\n",
    "from boto.s3.key import Key\n",
    "\n",
    "k = Key(mybucket)\n",
    "\n",
    "k.key = 'RC_2015-01'\n",
    "\n",
    "k.get_contents_to_filename('reddit_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e85fe1bb25a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_contents_as_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/s3/key.pyc\u001b[0m in \u001b[0;36mget_contents_as_string\u001b[1;34m(self, headers, cb, num_cb, torrent, version_id, response_headers, encoding)\u001b[0m\n\u001b[0;32m   1780\u001b[0m         self.get_contents_to_file(fp, headers, cb, num_cb, torrent=torrent,\n\u001b[0;32m   1781\u001b[0m                                   \u001b[0mversion_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1782\u001b[1;33m                                   response_headers=response_headers)\n\u001b[0m\u001b[0;32m   1783\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/s3/key.pyc\u001b[0m in \u001b[0;36mget_contents_to_file\u001b[1;34m(self, fp, headers, cb, num_cb, torrent, version_id, res_download_handler, response_headers)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                 self.get_file(fp, headers, cb, num_cb, torrent=torrent,\n\u001b[0;32m   1649\u001b[0m                               \u001b[0mversion_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mversion_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               response_headers=response_headers)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def get_contents_to_filename(self, filename, headers=None,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/s3/key.pyc\u001b[0m in \u001b[0;36mget_file\u001b[1;34m(self, fp, headers, cb, num_cb, torrent, version_id, override_num_retries, response_headers)\u001b[0m\n\u001b[0;32m   1480\u001b[0m                                 \u001b[0mresponse_headers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse_headers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1481\u001b[0m                                 \u001b[0mhash_algs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1482\u001b[1;33m                                 query_args=None)\n\u001b[0m\u001b[0;32m   1483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1484\u001b[0m     def _get_file_internal(self, fp, headers=None, cb=None, num_cb=10,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/s3/key.pyc\u001b[0m in \u001b[0;36m_get_file_internal\u001b[1;34m(self, fp, headers, cb, num_cb, torrent, version_id, override_num_retries, response_headers, hash_algs, query_args)\u001b[0m\n\u001b[0;32m   1533\u001b[0m             \u001b[0mcb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1535\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1536\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1537\u001b[0m                 \u001b[0mdata_len\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/s3/key.pyc\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \"\"\"\n\u001b[0;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBufferSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/boto/connection.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_response\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mhttp_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPResponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[1;31m# fragmentation issues on many platforms.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m    239\u001b[0m                     \u001b[1;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                     self.__class__)\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Save file\n",
    "\n",
    "\n",
    "mm = build_model('reddit_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-a0892c6645d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'AdviceAnimals'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mm' is not defined"
     ]
    }
   ],
   "source": [
    "mm.docvecs.most_similar('AdviceAnimals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
